# Workflow for Data Processing
This folder stores scripts for data processing that take in raw fastq files to generate analysis-ready bam files. 

## Prepare - before you start
1. Create a project directory and the following subdirectories (`mkdir -p`):  
   `proc_scripts` for storing scripts
   `raw_fastq` has the raw fastq files
   `sample_lists` for storing sample tables, sample lists, and other small text files
   `reference` contains reference genome file and a list of adapter sequences
   `adapter_clipped` for storing adapter clipped fastq files
   `qual_filtered`
   `mt_mapped`
   `bam_mtDNA`
   `bam` for storing alignment files
   `fastqc` for storing FastQC output
2. Prepare sample lists
   This is simply a list of prefixes for the samples we want to analyze. Our sample table can contain data for all individuals in our study, but at any given time, we may only want to perform an operation on a subset of them.   
   Note that itâ€™s just a list of fastq name prefixes, each on a separate line and there should be no header in this file.  
   `split_list.sh` is used to split fastq_list.txt into subsets. 
   - Note: in ~/sample_lists, check whether any lines have CRLF: `file * | grep CRLF`. If they do, get rid of those pesky line-terminators: `dos2unix sample_list_lane_2.txt`
3. Prepare a sample table
   The sample table has to be a tab deliminated table with the following six columns, strictly in this order:
      1. `prefix` the prefix of raw fastq file names
      2. `lane_number` each sequencing lane or batch should be assigned a unique identifier
      3. `seq_id` sequence ID; this variable is only relevant when different libraries were prepared out of the same sample and were run in the same lane (e.g. if you wanted to include a replicate). In this case, seq_id should be used to distinguish these separate libraries. If you only have a single library prepared from each of your samples (even if you sequence that same library across multiple lanes), you can just put 1 for all samples in this column.
      4. `sample_id` sample ID; a unique identifier for each individual sequenced
      5. `population` population name; the population or other relevant grouping variable that the individual belongs to
      6. `data_type` data type; there can only be two possible entries: pe (for paired-end data) or se (for single end data). We need this in the table because for some of our processing steps, the commands are slightly different for paired-end and single-end data.
   It is important to make sure that the combination of `sample_id`, `seq_id`, and `lane_number` is unique for each fastq file.
4. Find the program paths on the server

## Steps
1. `01_fastqc`: Evaluate overall data quality using FastQC.
   Always make sure to look both at the forward and reverse files because sometimes issues can arise in only one of the read directions.

2. `02_adapter_trimming`: Use Trimmomatic to trim and crop pair-end fastq, as well as to remove adapters. It is recommended in most cases that adapter clipping, if required, is done as early as possible, since correctly identifying adapters using partial matches is more difficult.

3. `03_filtering_polyg`: Trim polyg tail or low quality tail with fastp. 
   - fastp manual: https://github.com/OpenGene/fastp 
   - The fastp can cut low quality bases for per read in its 5' and 3' by evaluating the mean quality from a sliding window (like Trimmomatic but faster). 

4. `04_build_ref`: Build bowtie reference index. This only needs to be done once with the same reference genome to generate the `.dict` and `.fai` files. 

5. `05_mapping_mtDNA`: Map the paired-end reads to the mitochondrial DNA (ref_C_virginica-3.0_mtDNA.fasta.gz, here renamed as cv30_mtDNA) using Bowtie2 and only retain the PE reads that **fail to align concordantly to the mtDNA genome** in the mt_mapped directory. 

6. `06_mapping`: Map the retained PE reads in the mt_mapped directory to the nuclear genome using Bowtie2. Convert to bam file for storage, and add `view -q 20` filter here. This is meant to filter bam files to remove poorly mapped reads (non-unique mappings and mappings with a quality score < 20). 

7. `07_merge`: Merge bam files from different batchs (first and rerun datasets). Samples may be sequenced multiple times on different lanes (or within the same lane, with different barcodes). Here sequencing files are named as `$SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'_minq20_sorted.bam`. 
    - The R script generates a merge_bams.sh script from the fastq-level sample table and list. Since we will be working on bam files rather than fastq files downstream from this point, this R script also generates bam table and list, where we have bam IDs rather than fastq IDs in the sample lists that specify which samples we want to loop over in a particular pipeline step. 
    - To execute the merging step, run the bash script `merge_bam.sh` generated by the R script. If there are no files to merge, run `generate_bam_list.sh` to generate the bam list. 

8.  `08_dup_clipping`: Remove duplicate reads using Picard Tools MarkDuplicates and trim the overlapping part of each read pair in pair-end data using BamUtil clipOverlap. It is important to deduplicate after merging, because PCR duplicates for the same sample may exist in different lanes.

9.  `09_realign`: Realign reads around indels using GATK, creating target intervals across all bam files and used that for realignment. This needs to be run in the /bam/ folder so that the realigned bam files can be outputted to the correct directory. 

10. `10_summary`: Estimate read depth in your bam files & summarize other statistics. After all the filtering steps, we want to know what final depth of coverage we have for each samples for downstream analysis. 
    - We will use samtools depth to first compute the read depth at each bp position in the genome. Then we will pull the output file and compute depth summary stats in R. 

11. `11_downsample`: optional step. This downsampling was only used for the high coverage WGS data obtained from another project. 

